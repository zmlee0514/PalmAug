{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition \n",
    "1. Open recognition.ipynb  \n",
    "2. Execute cells in sequence  \n",
    "3. Choosing the pretrained model with the dataset and model setting shows in the model name  \n",
    "\n",
    "> If the pretrained model name contains a prefix \"rotation\", execting the cell \"rotation-based oversampling\". Otherwise, do not execute it.  \n",
    "\n",
    "## Matching algorithm\n",
    "1. Single: Executing \"multi-transform matching\" cell without any additional transformations.  \n",
    "2. Mirror-concatenated: Executing \"mirror-concatenated matching\" cell.  \n",
    "3. Multi-transform: Executing \"multi-transform matching\" cell with any additional transformations you want.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1638032706135,
     "user": {
      "displayName": "李政旻",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03343348249209325544"
     },
     "user_tz": -480
    },
    "id": "EdZf3JNNGa8Z",
    "outputId": "a13e1932-a7e6-48a4-ded4-74024eee23b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 29 21:22:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   44C    P8    42W / 320W |    684MiB / 10240MiB |     38%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1813      G   /usr/lib/xorg/Xorg                 59MiB |\n",
      "|    0   N/A  N/A      2261      G   /usr/lib/xorg/Xorg                259MiB |\n",
      "|    0   N/A  N/A      2391      G   /usr/bin/gnome-shell               52MiB |\n",
      "|    0   N/A  N/A      2408      G   ...mviewer/tv_bin/TeamViewer        3MiB |\n",
      "|    0   N/A  N/A      4274      G   /usr/lib/firefox/firefox          272MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timm\n",
    "import cv2\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "plt.style.use(\"default\")\n",
    "    \n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1., amplitude=(0,25)):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.amplitude = amplitude\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img_arr = np.asarray(img)\n",
    "        gauss = np.random.normal(self.mean,self.std,img_arr.shape) * random.randint(*self.amplitude)\n",
    "        img = img_arr + gauss\n",
    "        img[img>255] = 255\n",
    "        return Image.fromarray(img.astype(\"uint8\")).convert(\"RGB\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "batch_size_train = 32\n",
    "batch_size_test = 32\n",
    "image_size = 224\n",
    "\n",
    "# baseline augmentation\n",
    "# trainingTransform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.RandomApply([transforms.ColorJitter(brightness=0.5)], p=0.6),\n",
    "#     transforms.RandomApply([transforms.ColorJitter(contrast=0.5)], p=0.6),\n",
    "#     # transforms.RandomApply([transforms.ColorJitter(saturation=0.25)], p=0.6),\n",
    "#     transforms.RandomApply([transforms.ColorJitter(hue=0.25)], p=0.6),\n",
    "#     transforms.RandomApply([transforms.RandomChoice([transforms.GaussianBlur(1), transforms.GaussianBlur(3), transforms.GaussianBlur(5)])], p=0.6),\n",
    "#     transforms.RandomAdjustSharpness(1.5, p=0.6),\n",
    "#     transforms.RandomApply([transforms.RandomResizedCrop(size=image_size, scale=(1.07, 1.14))], p=0.6),\n",
    "#     transforms.RandomApply([transforms.RandomRotation([-5,5], expand=False)], p=0.6),\n",
    "#     # transforms.RandomApply([transforms.RandomAffine([-5,5], translate=(0.01, 0.01))], p=0.6),\n",
    "#     transforms.Resize(image_size),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# testingTransform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Resize(image_size)\n",
    "# ])\n",
    "\n",
    "\n",
    "    \n",
    "# Define space search for training settings\n",
    "from operator import itemgetter\n",
    "(brightness,\n",
    "brightness_p,\n",
    "contrast,\n",
    "contrast_p,\n",
    "# saturation,\n",
    "# saturation_p,\n",
    "hue,\n",
    "hue_p,\n",
    "# blur_sigma,\n",
    "# blur_p,\n",
    "noise_amp,\n",
    "noise_p,\n",
    "# rotation_degree,\n",
    "# rotation_p,\n",
    "# translatation,\n",
    "# translatation_p\n",
    ") = itemgetter(\n",
    "    'brightness',\n",
    "    'brightness_probability',\n",
    "    'contrast',\n",
    "    'contrast_probability',\n",
    "    # 'saturation',\n",
    "    # 'saturation_probability',\n",
    "    'hue',\n",
    "    'hue_probability',\n",
    "    # 'blur_sigma',\n",
    "    # 'blur_probability',\n",
    "    'noise_amplitude',\n",
    "    'noise_probability',\n",
    "    # 'rotation_degree',\n",
    "    # 'rotation_probability',\n",
    "    # 'translatation',\n",
    "    # 'translatation_probability',\n",
    ")(torch.load(\"optuna/best_params-reorder-third-50.pt\"))\n",
    "\n",
    "trainingTransform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=brightness)], p=brightness_p),\n",
    "    transforms.RandomApply([transforms.ColorJitter(contrast=contrast)], p=contrast_p),\n",
    "    # transforms.RandomApply([transforms.ColorJitter(saturation=saturation)], p=saturation_p),\n",
    "    transforms.RandomApply([transforms.ColorJitter(hue=hue)], p=hue_p),\n",
    "    # transforms.RandomApply([transforms.RandomChoice([transforms.GaussianBlur(3, sigma=(0.1, blur_sigma)), \n",
    "    #                                                  transforms.GaussianBlur(5, sigma=(0.1, blur_sigma))])], p=blur_p),\n",
    "    transforms.RandomApply([AddGaussianNoise(0 , 1, (0, noise_amp))], p=noise_p),\n",
    "    # transforms.RandomApply([transforms.RandomAffine(degrees=rotation_degree)], p=rotation_p),\n",
    "    # transforms.RandomApply([transforms.RandomAffine(0, translate=(translatation, translatation))], p=translatation_p),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testingTransform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6045,
     "status": "ok",
     "timestamp": 1638159694721,
     "user": {
      "displayName": "李政旻",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03343348249209325544"
     },
     "user_tz": -480
    },
    "id": "VaHbi_8eHp9b"
   },
   "outputs": [],
   "source": [
    "## Tongji dataset\n",
    "# contain both session of specific indices\n",
    "class TongjiTrainingDataset(Dataset):\n",
    "    '''\n",
    "    all images of selected indices \n",
    "    '''\n",
    "    def __init__(self, root, indices, transforms):\n",
    "        # 圖片所在的資料夾\n",
    "        self.root = root\n",
    "        # 需要的類別編號\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            for i in range(c*10, c*10+10):\n",
    "                self.fnames.append(os.path.join(self.root, 'session1/{:05d}.tiff'.format(i+1)))\n",
    "                self.fnames.append(os.path.join(self.root, 'session2/{:05d}.tiff'.format(i+1)))\n",
    "                # 左右手視為不同的類別\n",
    "                self.labels.append(c)\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 利用路徑讀取圖片\n",
    "        img = Image.open(self.fnames[idx])\n",
    "        # 將輸入的圖片轉換成符合預訓練模型的形式\n",
    "        img = self.transforms(img)\n",
    "        # 補足3個channel\n",
    "        # img = img.repeat(3,1,1)\n",
    "        # 圖片相對應的 label\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "# contain first session of all indices, and second session of not selected\n",
    "class TongjiTuningDataset(Dataset):\n",
    "    '''\n",
    "    testing set include half of the select indices, and this is the remain(useless)\n",
    "    '''\n",
    "    def __init__(self, root, indices, transforms):\n",
    "        self.root = root\n",
    "        # 註冊的類別編號\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for i in range(6000):\n",
    "            c = int(i/10)\n",
    "            self.fnames.append(os.path.join(self.root, 'session1/{:05d}.tiff'.format(i+1)))\n",
    "            self.labels.append(c)\n",
    "            if c not in self.indices:\n",
    "                self.fnames.append(os.path.join(self.root, 'session2/{:05d}.tiff'.format(i+1)))\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.fnames[idx])\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "# only contain one session\n",
    "class TongjiTestingDataset(Dataset):\n",
    "    '''\n",
    "    half of the selected indices\n",
    "    '''\n",
    "    def __init__(self, root, indices, mode, transforms):\n",
    "        # 圖片所在的資料夾\n",
    "        if mode == \"probe\":\n",
    "            self.root = os.path.join(root, \"session2\")\n",
    "        else:\n",
    "            self.root = os.path.join(root, \"session1\")\n",
    "        # self.root = root\n",
    "        # 需要的類別編號\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            for i in range(c*10, c*10+10):\n",
    "                self.fnames.append(os.path.join(self.root, '{:05d}.tiff'.format(i+1)))\n",
    "                # 左右手視為不同的類別\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 利用路徑讀取圖片\n",
    "        img = Image.open(self.fnames[idx])\n",
    "        # 將輸入的圖片轉換成符合預訓練模型的形式\n",
    "        img = self.transforms(img)\n",
    "        # 補足3個channel\n",
    "        # img = img.repeat(3,1,1)\n",
    "        # 圖片相對應的 label\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "# flexibly spliting support set and query set\n",
    "class TongjiFewShotDataset(Dataset):\n",
    "    '''\n",
    "    mode == gallery, get num_samples start from 1 to 20 of each class\n",
    "    mode == probe, get num_samples start from 20 to 1 of each class\n",
    "    '''\n",
    "    def __init__(self, root, indices, num_samples, mode, transforms):\n",
    "        # 圖片所在的資料夾\n",
    "        self.root = root\n",
    "        # 需要的類別編號\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "        if num_samples > 20:\n",
    "            raise BaseException(\"Number of samples larger than the limit\")\n",
    "        else:\n",
    "            self.session1 = 10 if num_samples > 10 else num_samples\n",
    "            self.session2 = num_samples - self.session1\n",
    "            if mode == \"probe\":\n",
    "                self.session1, self.session2 = self.session2, self.session1\n",
    "                self.session1 = range(10-self.session1, 10)\n",
    "                self.session2 = range(10-self.session2, 10)\n",
    "            else:\n",
    "                self.session1 = range(self.session1)\n",
    "                self.session2 = range(self.session2)\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            # get images from session1\n",
    "            for i in self.session1: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, 'session1/{:05d}.tiff'.format(c*10+i+1)))\n",
    "                self.labels.append(c)\n",
    "            # get images from session2\n",
    "            for i in self.session2: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, 'session2/{:05d}.tiff'.format(c*10+i+1)))\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 利用路徑讀取圖片\n",
    "        img = Image.open(self.fnames[idx])\n",
    "        img = np.asarray(img)\n",
    "        # 將輸入的圖片轉換成符合預訓練模型的形式\n",
    "        img = self.transforms(img)\n",
    "        # 補足3個channel\n",
    "        # img = img.repeat(3,1,1)\n",
    "        # 圖片相對應的 label\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "# flexibly spliting support set and query set\n",
    "class TongjiRotationCopyDataset(Dataset):\n",
    "    '''\n",
    "    mode == gallery, get num_samples start from 1 to 20 of each class\n",
    "    mode == probe, get num_samples start from 20 to 1 of each class\n",
    "    '''\n",
    "    def __init__(self, root, indices, num_samples, mode, transforms):\n",
    "        # 圖片所在的資料夾\n",
    "        self.root = root\n",
    "        # 需要的類別編號\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "        self.num_samples = num_samples\n",
    "        if num_samples > 20:\n",
    "            raise BaseException(\"Number of samples larger than the limit\")\n",
    "        else:\n",
    "            self.session1 = 10 if num_samples > 10 else num_samples\n",
    "            self.session2 = num_samples - self.session1\n",
    "            if mode == \"probe\":\n",
    "                self.session1, self.session2 = self.session2, self.session1\n",
    "                self.session1 = range(10-self.session1, 10)\n",
    "                self.session2 = range(10-self.session2, 10)\n",
    "            else:\n",
    "                self.session1 = range(self.session1)\n",
    "                self.session2 = range(self.session2)\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            # get images from session1\n",
    "            for i in self.session1: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, 'session1/{:05d}.tiff'.format(c*10+i+1)))\n",
    "                self.labels.append(c)\n",
    "            # get images from session2\n",
    "            for i in self.session2: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, 'session2/{:05d}.tiff'.format(c*10+i+1)))\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx % (len(self.indices) * self.num_samples)\n",
    "        quotient = int(idx / (len(self.indices) * self.num_samples))\n",
    "        img = Image.open(self.fnames[i])\n",
    "        img = np.asarray(img)\n",
    "        if quotient > 0:\n",
    "            img = np.rot90(img, quotient, (0,1)) # will rotate 1,2,3 times\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[i]\n",
    "        return img, label + 600 * quotient\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## PolyU dataset\n",
    "# contain both session of specific indices\n",
    "class PolyUTrainingDataset(Dataset):\n",
    "    '''\n",
    "    all images of selected indices \n",
    "    '''\n",
    "    def __init__(self, root, indices, transforms):\n",
    "        # 圖片所在的資料夾\n",
    "        self.root = root\n",
    "        # 需要的類別編號\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.fnames = [[],[],[]]  # R,G,B\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            for i in range(6):\n",
    "                self.fnames[0].append(os.path.join(self.root, 'Multispectral_R/{:03d}/1_{:02d}_s.bmp'.format(c+1, i+1)))\n",
    "                self.fnames[1].append(os.path.join(self.root, 'Multispectral_G/{:03d}/1_{:02d}_s.bmp'.format(c+1, i+1)))\n",
    "                self.fnames[2].append(os.path.join(self.root, 'Multispectral_B/{:03d}/1_{:02d}_s.bmp'.format(c+1, i+1)))\n",
    "                self.fnames[0].append(os.path.join(self.root, 'Multispectral_R/{:03d}/2_{:02d}_s.bmp'.format(c+1, i+1)))\n",
    "                self.fnames[1].append(os.path.join(self.root, 'Multispectral_G/{:03d}/2_{:02d}_s.bmp'.format(c+1, i+1)))\n",
    "                self.fnames[2].append(os.path.join(self.root, 'Multispectral_B/{:03d}/2_{:02d}_s.bmp'.format(c+1, i+1)))\n",
    "                # 2 sessions\n",
    "                self.labels.append(c)\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_R = Image.open(self.fnames[0][idx])\n",
    "        img_G = Image.open(self.fnames[1][idx])\n",
    "        img_B = Image.open(self.fnames[2][idx])\n",
    "        img = np.dstack((img_R,img_G,img_B))\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# only contain one session\n",
    "class PolyUTestingDataset(Dataset):\n",
    "    '''\n",
    "    half of the selected indices\n",
    "    '''\n",
    "    def __init__(self, root, indices, mode, transforms):\n",
    "        self.root = root\n",
    "        # 需要的類別編號\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "        # 決定session\n",
    "        if mode == \"probe\":\n",
    "            self.session = 2\n",
    "        else:\n",
    "            self.session = 1\n",
    "\n",
    "        self.fnames = [[],[],[]]  # R,G,B\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            for i in range(6):\n",
    "                self.fnames[0].append(os.path.join(self.root, 'Multispectral_R/{:03d}/{}_{:02d}_s.bmp'.format(c+1, self.session, i+1)))\n",
    "                self.fnames[1].append(os.path.join(self.root, 'Multispectral_G/{:03d}/{}_{:02d}_s.bmp'.format(c+1, self.session, i+1)))\n",
    "                self.fnames[2].append(os.path.join(self.root, 'Multispectral_B/{:03d}/{}_{:02d}_s.bmp'.format(c+1, self.session, i+1)))\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_R = Image.open(self.fnames[0][idx])\n",
    "        img_G = Image.open(self.fnames[1][idx])\n",
    "        img_B = Image.open(self.fnames[2][idx])\n",
    "        img = np.dstack((img_R,img_G,img_B))\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# flexibly spliting support set and query set\n",
    "class PolyUFewShotDataset(Dataset):\n",
    "    '''\n",
    "    mode == gallery, get num_samples start from 1 to 20 of each class\n",
    "    mode == probe, get num_samples start from 20 to 1 of each class\n",
    "    '''\n",
    "    def __init__(self, root, indices, num_samples, mode, transforms):\n",
    "        self.root = root\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "        if num_samples > 12:\n",
    "            raise BaseException(\"Number of samples larger than the limit\")\n",
    "        else:\n",
    "            session1 = 6 if num_samples > 6 else num_samples\n",
    "            session2 = num_samples - session1\n",
    "            if mode == \"probe\":\n",
    "                session1, session2 = session2, session1\n",
    "                self.session1 = range(6-session1, 6)\n",
    "                self.session2 = range(6-session2, 6)\n",
    "            else:\n",
    "                self.session1 = range(session1)\n",
    "                self.session2 = range(session2)\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            # get images from session1\n",
    "            for i in self.session1:\n",
    "                fname = []\n",
    "                for channel in \"RGB\":\n",
    "                    fname.append(os.path.join(self.root, 'Multispectral_{}/{:03d}/{}_{:02d}_s.bmp'.format(channel, c+1, 1, i+1)))\n",
    "                self.fnames.append(fname)\n",
    "                self.labels.append(c)\n",
    "            # get images from session2\n",
    "            for i in self.session2:\n",
    "                fname = []\n",
    "                for channel in \"RGB\":\n",
    "                    fname.append(os.path.join(self.root, 'Multispectral_{}/{:03d}/{}_{:02d}_s.bmp'.format(channel, c+1, 2, i+1)))\n",
    "                self.fnames.append(fname)\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgs = []\n",
    "        for i in self.fnames[idx]:\n",
    "            imgs.append(Image.open(i))\n",
    "        img = np.dstack(imgs)\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "# flexibly spliting support set and query set\n",
    "class PolyURotationCopyDataset(Dataset):\n",
    "    '''\n",
    "    mode == gallery, get num_samples start from 1 to 20 of each class\n",
    "    mode == probe, get num_samples start from 20 to 1 of each class\n",
    "    '''\n",
    "    def __init__(self, root, indices, num_samples, mode, transforms):\n",
    "        self.root = root\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "        self.num_samples = num_samples\n",
    "        if num_samples > 12:\n",
    "            raise BaseException(\"Number of samples larger than the limit\")\n",
    "        else:\n",
    "            session1 = 6 if num_samples > 6 else num_samples\n",
    "            session2 = num_samples - session1\n",
    "            if mode == \"probe\":\n",
    "                session1, session2 = session2, session1\n",
    "                self.session1 = range(6-session1, 6)\n",
    "                self.session2 = range(6-session2, 6)\n",
    "            else:\n",
    "                self.session1 = range(session1)\n",
    "                self.session2 = range(session2)\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            # get images from session1\n",
    "            for i in self.session1:\n",
    "                fname = []\n",
    "                for channel in \"RGB\":\n",
    "                    fname.append(os.path.join(self.root, 'Multispectral_{}/{:03d}/{}_{:02d}_s.bmp'.format(channel, c+1, 1, i+1)))\n",
    "                self.fnames.append(fname)\n",
    "                self.labels.append(c)\n",
    "            # get images from session2\n",
    "            for i in self.session2:\n",
    "                fname = []\n",
    "                for channel in \"RGB\":\n",
    "                    fname.append(os.path.join(self.root, 'Multispectral_{}/{:03d}/{}_{:02d}_s.bmp'.format(channel, c+1, 2, i+1)))\n",
    "                self.fnames.append(fname)\n",
    "                self.labels.append(c)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgs = []\n",
    "        i = idx % (len(self.indices) * self.num_samples)\n",
    "        quotient = int(idx / (len(self.indices) * self.num_samples))\n",
    "        for path in self.fnames[i]:\n",
    "            imgs.append(Image.open(path))\n",
    "        img = np.dstack(imgs)\n",
    "        if quotient > 0:\n",
    "            img = np.rot90(img, quotient, (0,1)) # will rotate 1,2,3 times\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[i]\n",
    "        return img, label + 500 * quotient\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MPD dataset\n",
    "# contain all data of specific phone\n",
    "class MPDTrainingDataset(Dataset):\n",
    "    '''\n",
    "    all images of selected indices \n",
    "    '''\n",
    "    def __init__(self, root, indices, phone, transforms):\n",
    "        self.root = root\n",
    "        self.indices = indices\n",
    "        self.phone = phone\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            for i in range(10):\n",
    "                for p in self.phone:\n",
    "                    self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 1, p, \"l\", i+1))) # 左手session1\n",
    "                    self.labels.append(2*c)\n",
    "                    self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 2, p, \"l\", i+1))) # 左手session2\n",
    "                    self.labels.append(2*c)\n",
    "                    self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 1, p, \"r\", i+1))) # 右手session1\n",
    "                    self.labels.append(2*c+1)\n",
    "                    self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 2, p, \"r\", i+1))) # 右手session2\n",
    "                    self.labels.append(2*c+1)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.fnames[idx])\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "# flexibly spliting support set and query set\n",
    "class MPDFewShotDataset(Dataset):\n",
    "    '''\n",
    "    can only use for single phone\n",
    "    mode == gallery, get num_samples start from 1 to 20 of each class\n",
    "    mode == probe, get num_samples start from 20 to 1 of each class\n",
    "    '''\n",
    "    def __init__(self, root, indices, num_samples, mode, transforms):\n",
    "        self.root = root\n",
    "        self.indices = indices\n",
    "        self.transforms = transforms\n",
    "        self.num_samples = num_samples\n",
    "        if num_samples > 40:\n",
    "            raise BaseException(\"Number of samples larger than the limit\")\n",
    "        else:\n",
    "            if mode == \"probe\": # count backward\n",
    "                self.session = [2,1]\n",
    "                self.phone = \"mh\"\n",
    "                self.samples = range(9, -1, -1)\n",
    "            else:\n",
    "                self.session = [1,2]\n",
    "                self.phone = \"hm\"\n",
    "                self.samples = range(10)\n",
    "\n",
    "        \n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            count = 0\n",
    "            for s in self.session:\n",
    "                for i in self.samples:\n",
    "                    for p in self.phone:\n",
    "                        if count >= self.num_samples:\n",
    "                            break\n",
    "                        self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, s, p, \"l\", i+1))) # 左手session1\n",
    "                        self.labels.append(2*c)\n",
    "                        self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, s, p, \"r\", i+1))) # 右手session1\n",
    "                        self.labels.append(2*c+1)\n",
    "                        count += 1\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.fnames[idx])\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "class MPDFewShotSingleDataset(Dataset):\n",
    "    '''\n",
    "    can only use for single phone\n",
    "    mode == gallery, get num_samples start from 1 to 20 of each class\n",
    "    mode == probe, get num_samples start from 20 to 1 of each class\n",
    "    '''\n",
    "    def __init__(self, root, indices, phone, num_samples, mode, transforms):\n",
    "        self.root = root\n",
    "        self.indices = indices\n",
    "        self.phone = phone\n",
    "        self.transforms = transforms\n",
    "        self.num_samples = num_samples\n",
    "        if num_samples > 20:\n",
    "            raise BaseException(\"Number of samples larger than the limit\")\n",
    "        else:\n",
    "            session1 = 10 if num_samples > 10 else num_samples\n",
    "            session2 = num_samples - session1\n",
    "            if mode == \"probe\": # count backward\n",
    "                session1, session2 = session2, session1\n",
    "                self.session1 = range(10-session1, 10)\n",
    "                self.session2 = range(10-session2, 10)\n",
    "            else:\n",
    "                self.session1 = range(session1)\n",
    "                self.session2 = range(session2)\n",
    "\n",
    "        \n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            # get images from session1\n",
    "            for i in self.session1: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 1, self.phone, \"l\", i+1))) # 左手session1\n",
    "                self.labels.append(2*c)\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 1, self.phone, \"r\", i+1))) # 右手session1\n",
    "                self.labels.append(2*c+1)\n",
    "            # get images from session2\n",
    "            for i in self.session2: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 2, self.phone, \"l\", i+1))) # 左手session2\n",
    "                self.labels.append(2*c)\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 2, self.phone, \"r\", i+1))) # 右手session2\n",
    "                self.labels.append(2*c+1)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.fnames[idx])\n",
    "        img = np.asarray(img)\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "class MPDFewShotSingleCopyDataset(Dataset):\n",
    "    '''\n",
    "    can only use for single phone\n",
    "    mode == gallery, get num_samples start from 1 to 20 of each class\n",
    "    mode == probe, get num_samples start from 20 to 1 of each class\n",
    "    '''\n",
    "    def __init__(self, root, indices, phone, num_samples, mode, transforms):\n",
    "        self.root = root\n",
    "        self.indices = indices\n",
    "        self.phone = phone\n",
    "        self.transforms = transforms\n",
    "        self.num_samples = num_samples\n",
    "        if num_samples > 20:\n",
    "            raise BaseException(\"Number of samples larger than the limit\")\n",
    "        else:\n",
    "            session1 = 10 if num_samples > 10 else num_samples\n",
    "            session2 = num_samples - session1\n",
    "            if mode == \"probe\": # count backward\n",
    "                session1, session2 = session2, session1\n",
    "                self.session1 = range(10-session1, 10)\n",
    "                self.session2 = range(10-session2, 10)\n",
    "            else:\n",
    "                self.session1 = range(session1)\n",
    "                self.session2 = range(session2)\n",
    "\n",
    "        \n",
    "        self.fnames = []\n",
    "        self.labels = []\n",
    "        for c in self.indices:\n",
    "            # get images from session1\n",
    "            for i in self.session1: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 1, self.phone, \"l\", i+1))) # 左手session1\n",
    "                self.labels.append(2*c)\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 1, self.phone, \"r\", i+1))) # 右手session1\n",
    "                self.labels.append(2*c+1)\n",
    "            # get images from session2\n",
    "            for i in self.session2: # [0,1,2,3,4,5,6,7,8,9]\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 2, self.phone, \"l\", i+1))) # 左手session2\n",
    "                self.labels.append(2*c)\n",
    "                self.fnames.append(os.path.join(self.root, '{:03d}_{}_{}_{}_{:02d}_ROI.jpeg'.format(c+1, 2, self.phone, \"r\", i+1))) # 右手session2\n",
    "                self.labels.append(2*c+1)\n",
    "        self.labels = torch.Tensor(self.labels).long()\n",
    "\n",
    "    def __getitem__(self, idx):    \n",
    "        i = idx % (len(self.indices)*2 * self.num_samples)\n",
    "        quotient = int(idx / (len(self.indices)*2 * self.num_samples))\n",
    "        img = Image.open(self.fnames[i])\n",
    "        img = np.asarray(img)\n",
    "        if quotient > 0:\n",
    "            img = np.rot90(img, quotient, (0,1)) # will rotate 1,2,3 times\n",
    "        img = self.transforms(img)\n",
    "        label = self.labels[i]\n",
    "        return img, label + 400 * quotient\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ResNet \n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0, bias=False):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes, eps=0.001)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# add batch norm\n",
    "class ResNet20_basic(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.conv1 = BasicConv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.conv2 = BasicConv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=1)\n",
    "        self.conv3 = BasicConv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1)\n",
    "        self.conv4 = BasicConv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc = nn.Linear(512*14*14 , num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None  \n",
    "        layers = []\n",
    "        layers.append(block(planes, planes, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)           # 112x112\n",
    "        x = self.layer1(x)          # \n",
    "        x = self.conv2(x)           # 56x56\n",
    "        x = self.layer2(x)          # \n",
    "        x = self.conv3(x)           # 28x28\n",
    "        x = self.layer3(x)          # \n",
    "        x = self.conv4(x)           # 14x14\n",
    "        x = self.layer4(x)          # \n",
    "\n",
    "        x = torch.flatten(x, 1)     # remove 1 X 1 grid and make vector of tensor shape \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def resnet20_basic(num_classes):\n",
    "    layers=[1, 2, 4, 1]\n",
    "    model = ResNet20_basic(BasicBlock, layers, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Loss functions\n",
    "def l2_norm(input, axis = 1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class CurricularFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s = 64., m = 0.5, centers=False):\n",
    "        super(CurricularFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.threshold = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer('t', torch.zeros(1))\n",
    "        nn.init.normal_(self.weight, std=0.01)\n",
    "\n",
    "    def classify(self, x, centers=None):   \n",
    "        weights = self.weight\n",
    "        if torch.is_tensor(centers):\n",
    "            weights = centers     \n",
    "        logits = F.linear(F.normalize(x), F.normalize(weights))\n",
    "        return self.s * logits\n",
    "\n",
    "    def forward(self, embbedings, label, centers=None):\n",
    "        weights = self.weight\n",
    "        if torch.is_tensor(centers) and centers.shape == self.weight.shape:\n",
    "            weights = centers\n",
    "        \n",
    "        embbedings = l2_norm(embbedings, axis = 1)\n",
    "        kernel_norm = l2_norm(weights, axis = 1)\n",
    "        cos_theta = torch.mm(embbedings, torch.transpose(kernel_norm, 0, 1))\n",
    "        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n",
    "        with torch.no_grad():\n",
    "            origin_cos = cos_theta.clone()\n",
    "        target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n",
    "\n",
    "        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
    "        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m #cos(target+margin)\n",
    "        mask = cos_theta > cos_theta_m\n",
    "        final_target_logit = torch.where(target_logit > self.threshold, cos_theta_m, target_logit - self.mm)\n",
    "\n",
    "        hard_example = cos_theta[mask]\n",
    "        with torch.no_grad():\n",
    "            self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t\n",
    "        cos_theta[mask] = hard_example * (self.t + hard_example)\n",
    "        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n",
    "        margin_output = cos_theta * self.s\n",
    "        original_logits = origin_cos * self.s\n",
    "        return margin_output, original_logits\n",
    "\n",
    "class ArcFace(torch.nn.Module):\n",
    "    \"\"\" ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=128, out_features=10575, s=32.0, m=0.50, easy_margin=False, centers=False):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.in_feature = in_features\n",
    "        self.out_feature = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "\n",
    "        # make the function cos(theta+m) monotonic decreasing while theta in [0°,180°]\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        # cos(theta)\n",
    "        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n",
    "        with torch.no_grad():\n",
    "            origin_cos = cosine.clone()\n",
    "        # cos(theta + m)\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n",
    "\n",
    "        #one_hot = torch.zeros(cosine.size(), device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output = output * self.s\n",
    "        margin_output = origin_cos * self.s\n",
    "        return margin_output, output\n",
    "    \n",
    "class LMCL(nn.Module):\n",
    "    def __init__(self, in_features=128, out_features=600, s=30.0, m=0.65, centers=False):\n",
    "        super(LMCL, self).__init__()\n",
    "        self.in_feature = in_features\n",
    "        self.out_feature = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        if torch.is_tensor(centers) and centers.shape == (out_features, in_features):\n",
    "            self.weight = nn.Parameter(centers)\n",
    "        else:\n",
    "            self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "            # nn.init.xavier_uniform_(self.weight)\n",
    "            # nn.init.kaiming_uniform_(self.weight)\n",
    "            # nn.init.normal_(self.weight, std=0.01)\n",
    "\n",
    "    def classify(self, x, centers=None):   \n",
    "        weights = self.weight\n",
    "        if torch.is_tensor(centers):\n",
    "            weights = centers     \n",
    "        logits = F.linear(F.normalize(x), F.normalize(weights))\n",
    "        return self.s * logits\n",
    "    \n",
    "    def forward(self, x, label, centers=None):\n",
    "        weights = self.weight\n",
    "        if torch.is_tensor(centers) and centers.shape == self.weight.shape:\n",
    "            weights = centers\n",
    "        # else:\n",
    "        #     weights = self.centers()\n",
    "        cosine = F.linear(F.normalize(x), F.normalize(weights))\n",
    "        with torch.no_grad():\n",
    "            origin_cos = cosine.clone()\n",
    "            \n",
    "        # one_hot = torch.zeros(cosine.size(), device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n",
    "\n",
    "        margin_output = self.s * (cosine - one_hot * self.m)\n",
    "        original_logits = self.s * origin_cos\n",
    "        return margin_output, original_logits\n",
    "    \n",
    "class CenterLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CenterLoss, self).__init__()\n",
    "\n",
    "    def forward(self, feats, label, centers):\n",
    "        center = centers[label]\n",
    "        dist = (feats-center).pow(2).sum(dim=-1) / 2\n",
    "        loss = torch.clamp(dist, min=1e-12, max=1e+12).mean(dim=-1)\n",
    "        return loss\n",
    "\n",
    "class CenterHuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(CenterHuberLoss, self).__init__()\n",
    "        self.HuberLoss = nn.HuberLoss(delta=delta)\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, feats, label, centers):\n",
    "        center = centers[label]\n",
    "        # dist = (feats-center).pow(2).sum(dim=-1) / 2\n",
    "        dist = self.HuberLoss(feats, center)\n",
    "        loss = torch.clamp(dist, min=1e-12, max=1e+12).mean(dim=-1)\n",
    "        return loss\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma = 2, eps = 1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {}\n",
    "dataset_params[\"Tongji\"] = [\"../data/TongJi/palmprint/ROI_RGB\", 600]\n",
    "dataset_params[\"PolyU\"] = [\"../data/PolyU\", 500]\n",
    "dataset_params[\"MPD\"] = [\"../data/MPD/ROI\", 200]\n",
    "dataset_params[\"MPD_h\"] = dataset_params[\"MPD_m\"] = dataset_params[\"MPD\"]\n",
    "\n",
    "def buildDatasets(dataset, shot=5, val_ratio=0.2, test_ratio=0.2, indices=False):\n",
    "    if indices:\n",
    "        training_class_indices, testing_class_indices = indices\n",
    "    elif test_ratio == 0:\n",
    "        training_class_indices = range(dataset_params[dataset][1])\n",
    "        testing_class_indices = range(dataset_params[dataset][1])\n",
    "    else:\n",
    "        training_class_indices, testing_class_indices = data.random_split(\n",
    "            range(dataset_params[dataset][1]), [int(dataset_params[dataset][1]*(1-test_ratio)), int(dataset_params[dataset][1]*test_ratio)])\n",
    "    \n",
    "    if(dataset == \"Tongji\"):\n",
    "        num_val_samples = int(20*val_ratio)\n",
    "        trainingDataset = TongjiFewShotDataset(dataset_params[dataset][0], training_class_indices, 20, \"gallery\", trainingTransform)\n",
    "        validationDataset = TongjiFewShotDataset(dataset_params[dataset][0], training_class_indices, 20, \"probe\", testingTransform)\n",
    "        galleryDataset = TongjiFewShotDataset(dataset_params[dataset][0], testing_class_indices, shot, \"gallery\", testingTransform)\n",
    "        probeDataset = TongjiFewShotDataset(dataset_params[dataset][0], testing_class_indices, 20-shot, \"probe\", testingTransform)\n",
    "    elif(dataset == \"PolyU\"):\n",
    "        num_val_samples = int(12*val_ratio)\n",
    "        trainingDataset = PolyUFewShotDataset(dataset_params[dataset][0], training_class_indices, 12, \"gallery\", trainingTransform)\n",
    "        validationDataset = PolyUFewShotDataset(dataset_params[dataset][0], training_class_indices, 12, \"probe\", testingTransform)\n",
    "        galleryDataset = PolyUFewShotDataset(dataset_params[dataset][0], testing_class_indices, shot, \"gallery\", testingTransform)\n",
    "        probeDataset = PolyUFewShotDataset(dataset_params[dataset][0], testing_class_indices, 12-shot, \"probe\", testingTransform)\n",
    "    elif(dataset == \"MPD_h\"):\n",
    "        num_val_samples = int(20*val_ratio)\n",
    "        trainingDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], training_class_indices, \"h\", 20, \"gallery\", trainingTransform)\n",
    "        validationDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], training_class_indices, \"h\", 20, \"probe\", testingTransform)\n",
    "        galleryDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], testing_class_indices, \"h\", shot, \"gallery\", testingTransform)\n",
    "        probeDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], testing_class_indices, \"h\", 20-shot, \"probe\", testingTransform)\n",
    "    elif(dataset == \"MPD_m\"):\n",
    "        num_val_samples = int(20*val_ratio)\n",
    "        trainingDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], training_class_indices, \"m\", 20, \"gallery\", trainingTransform)\n",
    "        validationDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], training_class_indices, \"m\", 20, \"probe\", testingTransform)\n",
    "        galleryDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], testing_class_indices, \"m\", shot, \"gallery\", testingTransform)\n",
    "        probeDataset = MPDFewShotSingleDataset(dataset_params[dataset][0], testing_class_indices, \"m\", 20-shot, \"probe\", testingTransform)\n",
    "    # elif(dataset == \"MPD\"):\n",
    "    #     trainingDataset = MPDTrainingDataset(dataset_params[dataset][0], training_class_indices, \"hm\", trainingTransform)\n",
    "    #     galleryDataset = MPDFewShotDataset(dataset_params[dataset][0], testing_class_indices, shot, \"gallery\", testingTransform)\n",
    "    #     probeDataset = MPDFewShotDataset(dataset_params[dataset][0], testing_class_indices, 40-shot, \"probe\", testingTransform)\n",
    "    else:\n",
    "        print(dataset)\n",
    "        \n",
    "    return trainingDataset, validationDataset, galleryDataset, probeDataset, training_class_indices, testing_class_indices\n",
    "\n",
    "def buildDataloaders(trainingDataset, validationDataset, galleryDataset, probeDataset, batch_size_train = 55, batch_size_test = 128):\n",
    "    trainingDataloader = DataLoader(trainingDataset, batch_size=batch_size_train, shuffle=True)\n",
    "    validationDataloader = DataLoader(validationDataset, batch_size=batch_size_test, shuffle=False)\n",
    "    galleryDataloader = DataLoader(galleryDataset, batch_size=batch_size_test, shuffle=False)\n",
    "    probeDataloader = DataLoader(probeDataset, batch_size=batch_size_test, shuffle=False)\n",
    "    return trainingDataloader, validationDataloader, galleryDataloader, probeDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "def initModel(model_type, head_type, num_classes, feature_dim, loss_func, lamb, lr=0.01, l2=0, mm=0.9, feature_norm=False):\n",
    "    if model_type == \"ResNet20_basic\":\n",
    "        model = resnet20_basic(feature_dim)\n",
    "    elif model_type == \"ResNet18\":\n",
    "        model = models.resnet18(pretrained = False)\n",
    "        model.avgpool = Identity()\n",
    "        model.fc = nn.Linear(model.fc.in_features*7*7, feature_dim) \n",
    "    elif model_type == \"ResNet18_default\":\n",
    "        model = models.resnet18(pretrained = False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"pretrained_ResNet18\":\n",
    "        model = models.resnet18(pretrained = True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"reduced_ResNet18\":\n",
    "        model = models.resnet18(pretrained = True)\n",
    "        model.layer4 = Identity()\n",
    "        model.fc = nn.Linear(256, feature_dim) \n",
    "    elif model_type == \"SE_ResNeXt26d_pretrained\":\n",
    "        model = timm.create_model('seresnext26d_32x4d', pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"SE_ResNeXt26d\":\n",
    "        model = timm.create_model('seresnext26d_32x4d')\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"ResNeSt26d_pretrained\":\n",
    "        model = timm.create_model('resnest26d', pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"Reduced_ResNeSt26d_pretrained\":\n",
    "        model = timm.create_model('resnest26d', pretrained=True)\n",
    "        model.layer4 = Identity()\n",
    "        model.fc = nn.Linear(1024, feature_dim) \n",
    "    elif model_type == \"Double_Reduced_ResNeSt26d_pretrained\":\n",
    "        model = timm.create_model('resnest26d', pretrained=True)\n",
    "        model.layer4 = Identity()\n",
    "        model.layer3 = Identity()\n",
    "        model.fc = nn.Linear(512, feature_dim) \n",
    "    elif model_type == \"ResNeSt26d\":\n",
    "        model = timm.create_model('resnest26d')\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"ResNeSt14d_pretrained\":\n",
    "        model = timm.create_model('resnest14d', pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"ResNeSt50d_pretrained\":\n",
    "        model = timm.create_model('resnest50d', pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, feature_dim) \n",
    "    elif model_type == \"Reduced_ResNeSt50d_pretrained\":\n",
    "        model = timm.create_model('resnest50d', pretrained=True)\n",
    "        model.layer4 = Identity()\n",
    "        model.fc = nn.Linear(1024, feature_dim)\n",
    "    elif model_type == \"Double_Reduced_ResNeSt50d_pretrained\":\n",
    "        model = timm.create_model('resnest50d', pretrained=True)\n",
    "        model.layer3 = Identity()\n",
    "        model.layer4 = Identity()\n",
    "        model.fc = nn.Linear(512, feature_dim) \n",
    "    else:\n",
    "        raise BaseException(\"Invalid model type\")\n",
    "\n",
    "    if head_type == \"LMCL\":\n",
    "        head = LMCL(in_features=feature_dim, out_features=num_classes, s=30.0, m=0.65, centers=False)\n",
    "        # head = LMCL_loss(num_classes, feature_dim, s=30.00, m=0.65)\n",
    "    elif head_type == \"CurricularFace\":\n",
    "        head = CurricularFace(in_features=feature_dim, out_features=num_classes, s=30.0, m=0.65, centers=False)\n",
    "    elif head_type == \"ArcFace\":\n",
    "        head = ArcFace(in_features=feature_dim, out_features=num_classes, s=30.0, m=0.65, centers=False)\n",
    "    else:\n",
    "        raise BaseException(\"Invalid loss function type\")\n",
    "        \n",
    "    if loss_func == \"CE+Center\":\n",
    "        criterion = {\"Softmax\": nn.CrossEntropyLoss(), \"CenterLoss\": CenterLoss(), \"Lambda\": lamb}\n",
    "    elif loss_func == \"Focal+Center\":\n",
    "        criterion = {\"Softmax\": FocalLoss(gamma=2), \"CenterLoss\": CenterLoss(), \"Lambda\": lamb}\n",
    "    elif loss_func == \"Focal+Huber\":\n",
    "        criterion = {\"Softmax\": FocalLoss(gamma=2), \"CenterLoss\": CenterHuberLoss(delta=12), \"Lambda\": lamb}\n",
    "    elif loss_func == \"CE+Huber\":\n",
    "        criterion = {\"Softmax\": nn.CrossEntropyLoss(), \"CenterLoss\": CenterHuberLoss(delta=10), \"Lambda\": lamb}\n",
    "    else:\n",
    "        raise BaseException(\"Invalid loss function type\")\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=mm)\n",
    "    optimizer4center = torch.optim.Adam(head.parameters(), lr=0.1, weight_decay=l2)\n",
    "    return model, head, criterion, optimizer, optimizer4center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ dataset parameters ------\n",
      "dataset:  PolyU\n",
      "val ratio:  0\n",
      "test ratio:  0.1\n",
      "number of shots:  5\n",
      "------ source dataset parameters ------\n",
      "training classes:  450\n",
      "testing classes:  50\n",
      "training samples:  5400\n",
      "validation samples:  5400\n",
      "gallery samples:  250\n",
      "probe samples:  350\n",
      "------ end dataset parameters ------\n"
     ]
    }
   ],
   "source": [
    "## initial dataset parameters\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "## dataset parameters\n",
    "dataset_choices = [\"PolyU\",\"Tongji\",\"MPD_h\",\"MPD_m\"]\n",
    "dataset = dataset_choices[0]\n",
    "val_ratio = 0\n",
    "test_ratio = 0.1\n",
    "shot = 5\n",
    "num_classes_source = dataset_params[dataset][1]*2 if dataset.startswith(\"MPD\") else dataset_params[dataset][1]\n",
    "dataset_type = f'{dataset}' if test_ratio else f'{dataset}_full'\n",
    "\n",
    "print(\"------ dataset parameters ------\")\n",
    "print(\"dataset: \", dataset)\n",
    "print(\"val ratio: \", val_ratio)\n",
    "print(\"test ratio: \", test_ratio)\n",
    "print(\"number of shots: \", shot)\n",
    "\n",
    "(trainingDataset_source, \n",
    "validationDataset_source, \n",
    "galleryDataset_source, \n",
    "probeDataset_source, \n",
    "training_class_indices_source, \n",
    "testing_class_indices_source) = buildDatasets(dataset, shot, val_ratio, test_ratio)\n",
    "trainingDataloader_source, validationDataloader_source, galleryDataloader_source, probeDataloader_source = buildDataloaders(\n",
    "    trainingDataset_source, validationDataset_source, galleryDataset_source, probeDataset_source, batch_size_train = 32, batch_size_test = 128)\n",
    "print(\"------ source dataset parameters ------\")\n",
    "print(\"training classes: \", len(training_class_indices_source))\n",
    "print(\"testing classes: \", len(testing_class_indices_source))\n",
    "print(\"training samples: \", len(trainingDataset_source))\n",
    "print(\"validation samples: \", len(validationDataset_source))\n",
    "print(\"gallery samples: \", len(galleryDataset_source))\n",
    "print(\"probe samples: \", len(probeDataset_source))\n",
    "\n",
    "print(\"------ end dataset parameters ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type += \"-optuna-third\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classes:  2000\n"
     ]
    }
   ],
   "source": [
    "## rotation-based oversampling\n",
    "trainingDataset_source = PolyURotationCopyDataset(\"../data/PolyU\", training_class_indices_source, 12, \"gallery\", trainingTransform)\n",
    "# trainingDataset_source = TongjiRotationCopyDataset(\"../data/TongJi/palmprint/ROI_RGB\", training_class_indices_source, 20, \"gallery\", trainingTransform)\n",
    "# trainingDataset_source = MPDFewShotSingleCopyDataset(\"../data/MPD/ROI\", training_class_indices_source, \"h\", 20, \"gallery\", trainingTransform)\n",
    "# trainingDataset_source = MPDFewShotSingleCopyDataset(\"../data/MPD/ROI\", training_class_indices_source, \"m\", 20, \"gallery\", trainingTransform)\n",
    "dataset_type += \"-rotation\"\n",
    "\n",
    "trainingDataloader_source = DataLoader(trainingDataset_source, batch_size=batch_size_train, shuffle=True)\n",
    "num_classes_source *= 4\n",
    "print(\"training classes: \", num_classes_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ model parameters ------\n",
      "model type:  Reduced_ResNeSt50d_pretrained\n",
      "loss function:  Focal+Huber , lambda:  1\n",
      "learning rate:  0.001 , L2 Norm(weight decay):  0 , momentum:  0.9\n",
      "sameple model name: PolyU-5s-Reduced_ResNeSt50d_pretrained-128emb-LMCL-Focal+Huber-1l-0.001lr-0.9mm-0l2-0e.pt\n",
      "parameter total:9412608, trainable:9412608\n",
      "------ end model parameters ------\n"
     ]
    }
   ],
   "source": [
    "## initial model parameters\n",
    "epoch = 0\n",
    "log_train_acc = []\n",
    "log_train_loss = []\n",
    "log_val_acc = []\n",
    "log_val_loss = []\n",
    "log_test_acc = []\n",
    "log_test_loss = []\n",
    "log_gradient_norm = []\n",
    "log_max_feature_norm = []\n",
    "\n",
    "feature_dim = 128\n",
    "## model type\n",
    "# model_type = \"ResNet20_basic\"\n",
    "# model_type = \"ResNet18\"\n",
    "# model_type = \"ResNet18_default\"\n",
    "# model_type = \"pretrained_ResNet18\"\n",
    "# model_type = \"reduced_ResNet18\"\n",
    "# model_type = \"SE_ResNeXt26d_pretrained\"\n",
    "# model_type = \"SE_ResNeXt26d\"\n",
    "# model_type = \"ResNeSt26d\"\n",
    "# model_type = \"ResNeSt26d_pretrained\"\n",
    "# model_type = \"Reduced_ResNeSt26d_pretrained\"\n",
    "# model_type = \"Double_Reduced_ResNeSt26d_pretrained\"\n",
    "# model_type = \"ResNeSt50d_pretrained\"\n",
    "model_type = \"Reduced_ResNeSt50d_pretrained\"\n",
    "# model_type = \"Double_Reduced_ResNeSt50d_pretrained\"\n",
    "\n",
    "## loss functinos\n",
    "head_type = \"LMCL\"\n",
    "# head_type = \"CurricularFace\"\n",
    "# head_type = \"ArcFace\"\n",
    "\n",
    "# loss_func = \"CE+Center\"\n",
    "# loss_func = \"Focal+Center\"\n",
    "loss_func = \"Focal+Huber\"\n",
    "# loss_func = \"CE+Huber\"\n",
    "# loss_func = \"CE+Circle\"\n",
    "lamb = 1\n",
    "\n",
    "# optimzer\n",
    "lr = 0.001\n",
    "mm = 0.9\n",
    "l2 = 0\n",
    "\n",
    "model, head, criterion, optimizer, optimzer4center = initModel(\n",
    "    model_type, head_type, num_classes_source, feature_dim, loss_func, lamb, lr, l2, mm)\n",
    "prefix = \"{}-{}s-{}-{}emb-{}-{}-{}l-{}lr-{}mm-{}l2\".format(dataset_type, shot, model_type, feature_dim, head_type, loss_func, lamb, lr, mm, l2)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"------ model parameters ------\")\n",
    "print(\"model type: \", model_type)\n",
    "print(\"loss function: \", loss_func, \", lambda: \", lamb)\n",
    "print(\"learning rate: \", lr, \", L2 Norm(weight decay): \", l2, \", momentum: \", mm)\n",
    "print(\"sameple model name: {}-{}e.pt\".format(prefix, epoch))\n",
    "print('parameter total:{}, trainable:{}'.format(total, trainable))\n",
    "print(\"------ end model parameters ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "checkpoint = torch.load(\"model/PolyU-optuna-third-5s-Reduced_ResNeSt50d_pretrained-128emb-LMCL-Focal+Huber-1l-0.001lr-0.9mm-0l2-20e.pt\")\n",
    "epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "head.load_state_dict(checkpoint['head_state_dict'])\n",
    "criterion = checkpoint['criterion']\n",
    "training_class_indices = checkpoint['training_class_indices']\n",
    "testing_class_indices = checkpoint['testing_class_indices']\n",
    "trainingTransform = checkpoint['trainingTransform']\n",
    "testingTransform = checkpoint['testingTransform']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ source dataset parameters ------\n",
      "training classes:  450\n",
      "testing classes:  50\n",
      "training samples:  5400\n",
      "validation samples:  5400\n",
      "gallery samples:  250\n",
      "probe samples:  350\n"
     ]
    }
   ],
   "source": [
    "val_ratio = 0\n",
    "(trainingDataset_source, \n",
    " validationDataset_source, \n",
    " galleryDataset_source, \n",
    " probeDataset_source, \n",
    " training_class_indices_source, \n",
    " testing_class_indices_source) = buildDatasets(\n",
    "    dataset, shot, val_ratio, test_ratio, (training_class_indices, testing_class_indices))\n",
    "trainingDataloader_source, validationDataloader_source, galleryDataloader_source, probeDataloader_source = buildDataloaders(\n",
    "    trainingDataset_source, validationDataset_source, galleryDataset_source, probeDataset_source, batch_size_train = 32, batch_size_test = 128)\n",
    "print(\"------ source dataset parameters ------\")\n",
    "print(\"training classes: \", len(training_class_indices_source))\n",
    "print(\"testing classes: \", len(testing_class_indices_source))\n",
    "print(\"training samples: \", len(trainingDataset_source))\n",
    "print(\"validation samples: \", len(validationDataset_source))\n",
    "print(\"gallery samples: \", len(galleryDataset_source))\n",
    "print(\"probe samples: \", len(probeDataset_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ dataset parameters ------\n",
      "dataset:  MPD_h\n",
      "val ratio:  0\n",
      "test ratio:  0\n",
      "number of shots:  5\n",
      "------ target dataset parameters ------\n",
      "training classes:  200\n",
      "testing classes:  200\n",
      "training samples:  8000\n",
      "validation samples:  8000\n",
      "gallery samples:  2000\n",
      "probe samples:  6000\n",
      "------ end dataset parameters ------\n"
     ]
    }
   ],
   "source": [
    "## target dataset parameters\n",
    "dataset_choices = [\"PolyU\",\"Tongji\",\"MPD_h\",\"MPD_m\"]\n",
    "dataset_target = dataset_choices[2]\n",
    "val_ratio = 0\n",
    "test_ratio = 0\n",
    "shot = 5\n",
    "num_classes_target = dataset_params[dataset_target][1]*2 if dataset_target.startswith(\"MPD\") else dataset_params[dataset_target][1]\n",
    "\n",
    "print(\"------ dataset parameters ------\")\n",
    "print(\"dataset: \", dataset_target)\n",
    "print(\"val ratio: \", val_ratio)\n",
    "print(\"test ratio: \", test_ratio)\n",
    "print(\"number of shots: \", shot)\n",
    "\n",
    "(trainingDataset_target, \n",
    "validationDataset_target, \n",
    "galleryDataset_target, \n",
    "probeDataset_target, \n",
    "training_class_indices_target, \n",
    "testing_class_indices_target) = buildDatasets(dataset_target, shot, val_ratio, test_ratio)\n",
    "trainingDataloader_target, validationDataloader_target, galleryDataloader_target, probeDataloader_target = buildDataloaders(\n",
    "    trainingDataset_target, validationDataset_target, galleryDataset_target, probeDataset_target, batch_size_train = 32, batch_size_test = 128)\n",
    "print(\"------ target dataset parameters ------\")\n",
    "print(\"training classes: \", len(training_class_indices_target))\n",
    "print(\"testing classes: \", len(testing_class_indices_target))\n",
    "print(\"training samples: \", len(trainingDataset_target))\n",
    "print(\"validation samples: \", len(validationDataset_target))\n",
    "print(\"gallery samples: \", len(galleryDataset_target))\n",
    "print(\"probe samples: \", len(probeDataset_target))\n",
    "\n",
    "print(\"------ end dataset parameters ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.900500\n",
      "597\n"
     ]
    }
   ],
   "source": [
    "# Mirror-concatenated matching\n",
    "model= model.cuda()\n",
    "model.eval()\n",
    "gallery_feature_loader = torch.Tensor().cuda()\n",
    "gallery_label_loader = torch.Tensor().long()\n",
    "probe_feature_loader = torch.Tensor().cuda()\n",
    "probe_label_loader = torch.Tensor().long()\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels) in enumerate(galleryDataloader_target):\n",
    "        inputs = img.cuda()\n",
    "        feats = model(inputs)\n",
    "        \n",
    "        # inputs_mirror = torch.fliplr(img).cuda()\n",
    "        inputs_mirror = torch.flip(img, (-2,)).cuda()\n",
    "        feats_mirror = model(inputs_mirror)\n",
    "        feats = torch.cat([feats, feats_mirror], 1)\n",
    "        \n",
    "        gallery_feature_loader = torch.cat([gallery_feature_loader, feats], 0)\n",
    "        gallery_label_loader = torch.cat([gallery_label_loader, labels], 0)\n",
    "    \n",
    "\n",
    "    for i, (img, labels) in enumerate(probeDataloader_target):\n",
    "        inputs = img.cuda()\n",
    "        feats = model(inputs)\n",
    "        \n",
    "        inputs_mirror = torch.flip(img, (-2,)).cuda()\n",
    "        feats_mirror = model(inputs_mirror)\n",
    "        # feats =  feats_mirror\n",
    "        feats = torch.cat([feats, feats_mirror], 1)\n",
    "        \n",
    "        probe_feature_loader = torch.cat([probe_feature_loader, feats], 0)\n",
    "        probe_label_loader = torch.cat([probe_label_loader, labels], 0)\n",
    "   \n",
    "\n",
    "match_scores = torch.Tensor()\n",
    "mathces = torch.Tensor()\n",
    "probe_label_loader = probe_label_loader.cpu()\n",
    "cos = nn.CosineSimilarity()\n",
    "test_acc = 0.0\n",
    "fail = 0\n",
    "fail_loader = []\n",
    "for i,p in enumerate(probe_feature_loader):\n",
    "    cosine = cos(p, gallery_feature_loader)\n",
    "    test_pred = torch.max(cosine, 0).indices.item()\n",
    "    test_acc += gallery_label_loader[test_pred] == probe_label_loader[i]\n",
    "    if gallery_label_loader[test_pred] != probe_label_loader[i]:\n",
    "        true_indexes = np.where(gallery_label_loader == probe_label_loader[i])\n",
    "        fail_loader.append([test_pred, i, cosine[test_pred].item(), cosine[true_indexes].cpu()])\n",
    "\n",
    "    # open-set verification: match scores for every gallery entry to every probe input\n",
    "    # match_scores = torch.cat([match_scores, cosine.cpu()], 0)\n",
    "    # mathces = torch.cat([mathces, (probe_label_loader[i] == gallery_label_loader)], 0)\n",
    "    \n",
    "test_acc /= len(probe_feature_loader)\n",
    "print(\"Test Acc: %3.6f\" % (test_acc))\n",
    "print(len(fail_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.852667\n",
      "Number of disabled gallery mirror template: 0\n",
      "Number of disabled probe mirror template: 0\n",
      "Number of failed: 884\n"
     ]
    }
   ],
   "source": [
    "# multi-transform matching\n",
    "model= model.cuda()\n",
    "model.eval()\n",
    "gallery_feature_loader = torch.Tensor().cuda()\n",
    "gallery_label_loader = torch.Tensor()\n",
    "probe_feature_loader = torch.Tensor().cuda()\n",
    "probe_label_loader = torch.Tensor()\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels) in enumerate(galleryDataloader_target):\n",
    "        images = []\n",
    "        images.append(img.cuda()) # original\n",
    "        # images.append(torch.flip(img, (-1,)).cuda()) # horizantal\n",
    "        # images.append(torch.flip(img, (-2,)).cuda()) # vertical\n",
    "        # images.append(torch.flip(img, (-1,-2)).cuda()) # h+v = 180 degree\n",
    "        # images.append(torch.rot90(img, 1, (-1,-2)).cuda()) # 90 degree\n",
    "        # images.append(torch.rot90(img, 3, (-1,-2)).cuda()) # 270 degree\n",
    "\n",
    "        feats = []\n",
    "        for image in images:\n",
    "            feats.append(model(image))\n",
    "        feats = torch.stack(feats, dim=1)\n",
    "        \n",
    "        gallery_feature_loader = torch.cat([gallery_feature_loader, feats], 0)\n",
    "        gallery_label_loader = torch.cat([gallery_label_loader, labels], 0)\n",
    "    for i, (img, labels) in enumerate(probeDataloader_target):\n",
    "        images = []\n",
    "        images.append(img.cuda()) # original\n",
    "        # images.append(torch.flip(img, (-1,)).cuda()) # horizantal\n",
    "        # images.append(torch.flip(img, (-2,)).cuda()) # vertical\n",
    "        # images.append(torch.flip(img, (-1,-2)).cuda()) # h+v = 180 degree\n",
    "        # images.append(torch.rot90(img, 1, (-1,-2)).cuda()) # 90 degree\n",
    "        # images.append(torch.rot90(img, 3, (-1,-2)).cuda()) # 270 degree\n",
    "\n",
    "        feats = []\n",
    "        for image in images:\n",
    "            feats.append(model(image))\n",
    "        feats = torch.stack(feats, dim=1)\n",
    "        \n",
    "        probe_feature_loader = torch.cat([probe_feature_loader, feats], 0)\n",
    "        probe_label_loader = torch.cat([probe_label_loader, labels], 0)\n",
    "\n",
    "match_scores = torch.Tensor()\n",
    "mathces = torch.Tensor()\n",
    "cos = nn.CosineSimilarity(dim=-1)\n",
    "test_acc = 0.0\n",
    "fail = 0\n",
    "fail_loader = []\n",
    "for i,p in enumerate(probe_feature_loader):\n",
    "    cosine = cos(p, gallery_feature_loader)\n",
    "    \n",
    "    # average all transformation type\n",
    "    similarity_mean = cosine.sum(dim=-1) / (cosine!=0).sum(1)\n",
    "    \n",
    "    test_pred = torch.max(similarity_mean, 0).indices.item()\n",
    "    test_acc += gallery_label_loader[test_pred] == probe_label_loader[i]\n",
    "    \n",
    "    if gallery_label_loader[test_pred] != probe_label_loader[i]:\n",
    "        true_indexes = np.where(gallery_label_loader == probe_label_loader[i])\n",
    "        fail_loader.append([test_pred, i, cosine[test_pred].cpu(), cosine[true_indexes].cpu()])\n",
    "        \n",
    "    # open-set verification: match scores for every gallery entry to every probe input\n",
    "    # match_scores = torch.cat([match_scores, similarity_mean.cpu()], 0)\n",
    "    # mathces = torch.cat([mathces, (probe_label_loader[i] == gallery_label_loader)], 0)\n",
    "    \n",
    "test_acc /= len(probe_feature_loader)\n",
    "print(\"Test Acc: %3.6f\" % (test_acc))\n",
    "print(\"Number of disabled gallery mirror template: %d\" % ((gallery_feature_loader.norm(dim=-1) == 0).sum()))\n",
    "print(\"Number of disabled probe mirror template: %d\" % ((probe_feature_loader.norm(dim=-1) == 0).sum()))\n",
    "print(\"Number of failed: %d\" % (len(fail_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(406, device='cuda:0'), 22, tensor(0.6359, device='cuda:0'), tensor([0.5847, 0.6338, 0.6024, 0.6092, 0.6068], device='cuda:0')]\n",
      "tensor(169.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(fail_loader[1])\n",
    "print(gallery_label_loader[fail_loader[0][0]], probe_label_loader[fail_loader[0][1]])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM9OYa7q08WehlLvmuNr7gR",
   "collapsed_sections": [],
   "name": "reproduce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "134771abc25f4d2a822528187760b585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20536ee0d4ae47389167014dad6ef3d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad1d226a52da482dab059aa7e43e0e08",
      "placeholder": "​",
      "style": "IPY_MODEL_41c2f6e242ab4cfe9cbdab6517d25c89",
      "value": "100%"
     }
    },
    "21de5756f9b84defbd3f862db5839b71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41c2f6e242ab4cfe9cbdab6517d25c89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63f5972f331f490394164a5daab809b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20536ee0d4ae47389167014dad6ef3d7",
       "IPY_MODEL_da182b666f394b02a51f5663dacd5512",
       "IPY_MODEL_e5eb4d83cb264e7681edc617843640f8"
      ],
      "layout": "IPY_MODEL_134771abc25f4d2a822528187760b585"
     }
    },
    "68e139184837463eafafc7405258b8cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69430c774bb349dbb9671e3655705c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad1d226a52da482dab059aa7e43e0e08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3e910b700ab47dfbab8b6e10c12585d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da182b666f394b02a51f5663dacd5512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68e139184837463eafafc7405258b8cf",
      "max": 22139423,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3e910b700ab47dfbab8b6e10c12585d",
      "value": 22139423
     }
    },
    "e5eb4d83cb264e7681edc617843640f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21de5756f9b84defbd3f862db5839b71",
      "placeholder": "​",
      "style": "IPY_MODEL_69430c774bb349dbb9671e3655705c01",
      "value": " 21.1M/21.1M [00:00&lt;00:00, 79.1MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
